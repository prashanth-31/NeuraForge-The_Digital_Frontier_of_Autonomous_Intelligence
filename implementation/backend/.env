# NeuraForge Environment Configuration

# Ollama Configuration
# Change this URL if your Ollama API is running on a different host/port
OLLAMA_BASE_URL=http://localhost:11434
# Make sure this matches exactly with the model name in Ollama
LLM_MODEL=llama3.1:8b

# API Server Configuration
API_HOST=127.0.0.1
API_PORT=8000
DEBUG=true

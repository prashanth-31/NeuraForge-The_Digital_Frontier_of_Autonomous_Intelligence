{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e041057d",
   "metadata": {},
   "source": [
    "# Phase 4 Playbook Demo\n",
    "\n",
    "This notebook demonstrates how the Phase 4 LangGraph agents collaborate using the tool wrapping and telemetry features implemented in the MCP layer. It builds a minimal orchestration run with in-memory stubs so you can validate behaviour without external services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61e478",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* Ensure the backend virtual environment is active (`.venv`).\n",
    "* Run `uvicorn app.main:app --reload` in a separate terminal if you want to compare the notebook flow with the live API.\n",
    "* Install optional visualization dependencies (`pip install rich`) if you prefer coloured console output (not required for this walkthrough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92443931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Awaitable, Callable\n",
    "\n",
    "from app.agents.base import AgentContext\n",
    "from app.agents.enterprise import EnterpriseAgent\n",
    "from app.agents.research import ResearchAgent\n",
    "from app.orchestration.graph import Orchestrator\n",
    "from app.services.tools import ToolInvocationResult\n",
    "\n",
    "\n",
    "class InMemoryStore:\n",
    "    def __init__(self) -> None:\n",
    "        self.records: dict[str, list[Any]] = {}\n",
    "\n",
    "    async def store_working_memory(self, task_id: str, payload: Any) -> None:\n",
    "        self.records.setdefault(task_id, []).append({\"working\": payload})\n",
    "\n",
    "    async def store_ephemeral_memory(self, task_id: str, payload: dict[str, Any]) -> None:\n",
    "        self.records.setdefault(task_id, []).append({\"ephemeral\": payload})\n",
    "\n",
    "\n",
    "class EchoLLM:\n",
    "    async def generate(self, prompt: str, *, system_prompt: str | None = None, temperature: float | None = None) -> str:\n",
    "        prefix = system_prompt or \"system\"\n",
    "        return f\"{prefix}: {prompt[:120]}\"\n",
    "\n",
    "    async def chat(self, messages: list[Any]) -> str:\n",
    "        return \"Chat flow not required for this demo.\"\n",
    "\n",
    "\n",
    "class DummyToolService:\n",
    "    async def invoke(self, tool: str, payload: dict[str, Any]) -> ToolInvocationResult:\n",
    "        if tool == \"research.search\":\n",
    "            response = {\n",
    "                \"results\": [\n",
    "                    {\"summary\": \"AI adoption can double GTM efficiency\", \"source\": \"https://example.com/ai-report\"},\n",
    "                    {\"summary\": \"Enterprise buyers prioritise governance\", \"source\": \"https://example.com/governance\"},\n",
    "                ]\n",
    "            }\n",
    "            return ToolInvocationResult(\n",
    "                tool=tool,\n",
    "                resolved_tool=\"search/tavily\",\n",
    "                payload=payload,\n",
    "                response=response,\n",
    "                cached=False,\n",
    "                latency=0.012,\n",
    "            )\n",
    "        if tool == \"enterprise.playbook\":\n",
    "            response = {\n",
    "                \"actions\": [\n",
    "                    {\"action\": \"Launch GTM readiness stand-up\", \"impact\": \"Align marketing, sales, and success stakeholders\", \"origin\": \"notion\"},\n",
    "                    {\"action\": \"Draft governance FAQ\", \"impact\": \"Address buyer risk narratives proactively\", \"origin\": \"policy_checker\"},\n",
    "                ]\n",
    "            }\n",
    "            return ToolInvocationResult(\n",
    "                tool=tool,\n",
    "                resolved_tool=\"enterprise/playbook\",\n",
    "                payload=payload,\n",
    "                response=response,\n",
    "                cached=False,\n",
    "                latency=0.018,\n",
    "            )\n",
    "        raise RuntimeError(f\"Unsupported tool alias: {tool}\")\n",
    "\n",
    "\n",
    "async def run_demo(progress_cb: Callable[[dict[str, Any]], Awaitable[None]] | None = None) -> dict[str, Any]:\n",
    "    orchestrator = Orchestrator(agents=[ResearchAgent(), EnterpriseAgent()])\n",
    "    memory = InMemoryStore()\n",
    "    context = AgentContext(\n",
    "        memory=memory,\n",
    "        llm=EchoLLM(),\n",
    "        context=None,\n",
    "        tools=DummyToolService(),\n",
    "        scorer=None,\n",
    "    )\n",
    "\n",
    "    task_state = {\n",
    "        \"id\": \"demo-001\",\n",
    "        \"prompt\": \"Summarize GTM guidance for the platform launch and translate it into an executive playbook.\",\n",
    "        \"metadata\": {\"segment\": \"enterprise\", \"priority\": \"high\"},\n",
    "    }\n",
    "\n",
    "    result = await orchestrator.route_task(task_state, context=context, progress_cb=progress_cb)\n",
    "    return {\"result\": result, \"memory_records\": memory.records}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225af396",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def log_progress(event: dict[str, Any]) -> None:\n",
    "    print(f\"[{event['agent']}] {event['event']} (latency={event.get('latency')}s)\")\n",
    "\n",
    "demo_payload = await run_demo(progress_cb=log_progress)\n",
    "demo_payload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3dadbe",
   "metadata": {},
   "source": [
    "## Observability Notes\n",
    "\n",
    "* The notebook uses the same `Orchestrator` and agent implementations that the FastAPI layer consumes.\n",
    "* Real tool invocations stream structured telemetry via `ToolService.instrument`; in this stubbed run we focus on the agent lifecycle hooks.\n",
    "* To connect notebook experiments with the live SSE stream, point a browser or CLI client at `/submit_task/stream` and compare the emitted `tool_invocation` events.\n",
    "\n",
    "Next steps:\n",
    "1. Swap the dummy services with instances created from `Settings` to run against local Redis/Qdrant/Ollama.\n",
    "2. Capture example outputs and push them into the docs repository for product demos.\n",
    "3. Extend the notebook with visualizations (e.g. confidence breakdown bar charts) once Prometheus metrics are exported."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
